"""
COMA è®­ç»ƒå™¨
åŸºäºQMIX trainerç»“æ„ï¼Œå®ç°COMAç®—æ³•çš„è®­ç»ƒæµç¨‹
"""
import os
import time
import numpy as np
import torch
import logging
from typing import Dict, Any, List, Optional
import json
from datetime import datetime
from collections import defaultdict

from src.algos import COMA
from src.models import COMANetworks
from src.buffer import ReplayBuffer
from src.utils import set_seed, get_device, load_config
from src.envs import create_ctde_env

logger = logging.getLogger(__name__)


class COMATrainer:
    """COMAè®­ç»ƒå™¨"""

    def __init__(self, config):
        """
        åˆå§‹åŒ–è®­ç»ƒå™¨

        Args:
            config: é…ç½®å­—å…¸æˆ–é…ç½®æ–‡ä»¶è·¯å¾„
        """
        if isinstance(config, str):
            # å¦‚æœæ˜¯é…ç½®æ–‡ä»¶è·¯å¾„ï¼ŒåŠ è½½é…ç½®
            self.config = load_config(config)
            self.config_path = config
        else:
            # å¦‚æœæ˜¯é…ç½®å­—å…¸ï¼Œç›´æ¥ä½¿ç”¨
            self.config = config
            self.config_path = None

        self.device = get_device()

        # è®¾ç½®éšæœºç§å­
        seed = int(time.time())
        set_seed(seed)

        # åˆ›å»ºç¯å¢ƒ
        self.env_name = self.config['env']['name']
        self.difficulty = self.config['env']['difficulty']
        self.global_state_type = self.config['env'].get('global_state_type', 'concat')

        self.env = create_ctde_env(
            env_name=self.env_name,
            difficulty=self.difficulty,
            global_state_type=self.global_state_type
        )

        # è·å–ç¯å¢ƒä¿¡æ¯
        self.env_info = self.env.get_env_info()
        logger.info(f"ç¯å¢ƒä¿¡æ¯: {self.env_info}")

        # åˆ›å»ºç½‘ç»œ
        self.networks = COMANetworks(self.env_info, self.config, self.device)

        # åˆ›å»ºç®—æ³•
        self.coma = COMA(self.networks, self.config, self.device)

        # åˆ›å»ºç»éªŒå›æ”¾ç¼“å†²åŒº
        self.buffer = ReplayBuffer(
            capacity=self.config['training']['buffer_size'],
            n_agents=self.env_info['n_agents'],
            obs_dim=max(self.env_info['obs_dims']) if isinstance(self.env_info['obs_dims'], list) else self.env_info['obs_dims'],
            state_dim=self.env_info['global_state_dim'],
            device=self.device
        )

        # è®­ç»ƒå‚æ•°
        self.total_episodes = self.config['training']['total_episodes']
        self.batch_size = self.config['training']['batch_size']
        self.warmup_episodes = self.config['training']['warmup_episodes']
        self.eval_interval = self.config['training']['eval_interval']
        self.save_interval = self.config['training']['save_interval']

        # æ¢ç´¢å‚æ•°
        self.epsilon_start = self.config['exploration']['epsilon_start']
        self.epsilon_end = self.config['exploration']['epsilon_end']
        self.epsilon_decay = self.config['exploration']['epsilon_decay']
        self.current_epsilon = self.epsilon_start

        # è®­ç»ƒçŠ¶æ€
        self.episode_count = 0
        self.learn_steps = 0

        # è®°å½•æŒ‡æ ‡
        self.episode_rewards = []
        self.episode_lengths = []
        self.losses = []
        self.eval_rewards = []
        self.eval_episodes = []
        self.epsilon_history = []

        # åˆ›å»ºä¿å­˜ç›®å½•
        self.save_dir = "checkpoints"
        self.plot_dir = "plots"
        os.makedirs(self.save_dir, exist_ok=True)
        os.makedirs(self.plot_dir, exist_ok=True)

        logger.info("COMAè®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆ")

    def _update_epsilon(self):
        """æ›´æ–°æ¢ç´¢ç‡"""
        self.current_epsilon = max(
            self.epsilon_end,
            self.current_epsilon * self.epsilon_decay
        )
        self.epsilon_history.append(self.current_epsilon)

    def _collect_experience(self) -> Dict[str, float]:
        """æ”¶é›†ä¸€ä¸ªepisodeçš„ç»éªŒ"""
        obs, info = self.env.reset()
        episode_reward = 0
        episode_length = 0

        # è·å–åˆå§‹å…¨å±€çŠ¶æ€
        if hasattr(info, 'get') and 'global_state' in info:
            global_state = info['global_state']
        else:
            # å¦‚æœæ²¡æœ‰æä¾›å…¨å±€çŠ¶æ€ï¼Œä½¿ç”¨è§‚æµ‹çš„æ‹¼æ¥
            global_state = np.concatenate([obs[agent_id] for agent_id in sorted(obs.keys())])

        done = False

        while not done:
            # é€‰æ‹©åŠ¨ä½œ
            actions = self.coma.select_actions(obs, self.current_epsilon)

            # æ‰§è¡ŒåŠ¨ä½œ
            step_result = self.env.step(actions)
            if len(step_result) == 4:
                next_obs, rewards, dones, next_info = step_result
                terminated = {agent_id: dones[agent_id] for agent_id in dones}
                truncated = {agent_id: False for agent_id in dones}
            else:
                next_obs, rewards, terminated, truncated, next_info = step_result

            # è·å–ä¸‹ä¸€ä¸ªå…¨å±€çŠ¶æ€
            if hasattr(next_info, 'get') and 'global_state' in next_info:
                next_global_state = next_info['global_state']
            else:
                next_global_state = np.concatenate([next_obs[agent_id] for agent_id in sorted(next_obs.keys())])

            # åˆ¤æ–­æ˜¯å¦ç»“æŸ
            done = any(terminated.values()) or any(truncated.values())
            dones = {agent_id: terminated[agent_id] or truncated[agent_id] for agent_id in terminated}

            # å­˜å‚¨ç»éªŒ
            self.buffer.push(
                obs=obs,
                actions=actions,
                rewards=rewards,
                next_obs=next_obs,
                dones=dones,
                global_state=global_state,
                next_global_state=next_global_state
            )

            # æ›´æ–°çŠ¶æ€
            obs = next_obs
            global_state = next_global_state

            # ç»Ÿè®¡
            episode_reward += sum(rewards.values())
            episode_length += 1

        return {
            'episode_reward': episode_reward,
            'episode_length': episode_length
        }

    def _evaluate(self, n_episodes: int = 5) -> float:
        """è¯„ä¼°å½“å‰ç­–ç•¥"""
        total_rewards = []

        for _ in range(n_episodes):
            obs, info = self.env.reset()
            episode_reward = 0
            done = False

            while not done:
                # è´ªå©ªç­–ç•¥é€‰æ‹©åŠ¨ä½œ
                actions = self.coma.select_actions(obs, epsilon=0.0)

                # æ‰§è¡ŒåŠ¨ä½œ
                step_result = self.env.step(actions)
                if len(step_result) == 4:
                    next_obs, rewards, dones, next_info = step_result
                    terminated = {agent_id: dones[agent_id] for agent_id in dones}
                    truncated = {agent_id: False for agent_id in dones}
                else:
                    next_obs, rewards, terminated, truncated, next_info = step_result

                # åˆ¤æ–­æ˜¯å¦ç»“æŸ
                done = any(terminated.values()) or any(truncated.values())

                # æ›´æ–°çŠ¶æ€
                obs = next_obs

                # ç»Ÿè®¡
                episode_reward += sum(rewards.values())

            total_rewards.append(episode_reward)

        return np.mean(total_rewards)

    def train(self):
        """ä¸»è®­ç»ƒå¾ªç¯"""
        logger.info(f"å¼€å§‹è®­ç»ƒCOMAç®—æ³•")
        logger.info(f"ç¯å¢ƒ: {self.env_name}, éš¾åº¦: {self.difficulty}")
        logger.info(f"æ€»episodes: {self.total_episodes}")

        start_time = time.time()

        for episode in range(self.total_episodes):
            self.episode_count = episode

            # æ”¶é›†ç»éªŒ
            episode_stats = self._collect_experience()

            # è®°å½•æŒ‡æ ‡
            self.episode_rewards.append(episode_stats['episode_reward'])
            self.episode_lengths.append(episode_stats['episode_length'])

            # æ›´æ–°æ¢ç´¢ç‡
            self._update_epsilon()

            # è®­ç»ƒç½‘ç»œ
            if len(self.buffer) >= self.batch_size and episode >= self.warmup_episodes:
                for _ in range(min(len(self.buffer) // self.batch_size, 10)):  # æ¯ä¸ªepisodeè®­ç»ƒå¤šæ¬¡
                    batch = self.buffer.sample(self.batch_size)
                    train_stats = self.coma.update(batch)
                    self.losses.append(train_stats['actor_loss'] + train_stats['critic_loss'])
                    self.learn_steps += 1

            # è¯„ä¼°
            if episode % self.eval_interval == 0 and episode > 0:
                eval_reward = self._evaluate()
                self.eval_rewards.append(eval_reward)
                self.eval_episodes.append(episode)

                logger.info(f"Episode {episode}: "
                           f"Reward={episode_stats['episode_reward']:.2f}, "
                           f"Eval={eval_reward:.2f}, "
                           f"Epsilon={self.current_epsilon:.3f}, "
                           f"Buffer={len(self.buffer)}")

            # ä¿å­˜æ¨¡å‹
            if episode % self.save_interval == 0 and episode > 0:
                self.save_model(episode)

            # å®šæœŸè¾“å‡ºè¿›åº¦
            if episode % 100 == 0:
                recent_rewards = self.episode_rewards[-100:] if len(self.episode_rewards) >= 100 else self.episode_rewards
                avg_reward = np.mean(recent_rewards)
                logger.info(f"Episode {episode}: "
                           f"Avg Reward (last 100)={avg_reward:.2f}, "
                           f"Epsilon={self.current_epsilon:.3f}, "
                           f"Learn Steps={self.learn_steps}")

        # è®­ç»ƒå®Œæˆï¼Œä¿å­˜æœ€ç»ˆæ¨¡å‹å’Œæ•°æ®
        self.save_model(self.total_episodes)
        self.save_training_data()

        end_time = time.time()
        training_time = end_time - start_time

        logger.info(f"è®­ç»ƒå®Œæˆ!")
        logger.info(f"æ€»è®­ç»ƒæ—¶é—´: {training_time/3600:.2f} å°æ—¶")
        logger.info(f"æœ€ç»ˆå¥–åŠ±: {self.episode_rewards[-1]:.2f}")
        logger.info(f"å¹³å‡å¥–åŠ± (last 100): {np.mean(self.episode_rewards[-100:]):.2f}")

        # ç”Ÿæˆå›¾è¡¨
        self.generate_plots()

        return self.episode_rewards, self.eval_rewards

    def save_model(self, episode: int):
        """ä¿å­˜æ¨¡å‹"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        model_path = os.path.join(
            self.save_dir,
            f"COMA_{self.env_name}_{self.difficulty}_episode_{episode}_{timestamp}.pt"
        )
        self.coma.save(model_path)
        logger.info(f"æ¨¡å‹å·²ä¿å­˜åˆ°: {model_path}")

    def save_training_data(self):
        """ä¿å­˜è®­ç»ƒæ•°æ® - ä¸QMIXæ ¼å¼å®Œå…¨ä¸€è‡´"""
        import os
        import json
        import numpy as np
        from datetime import datetime

        os.makedirs(self.save_dir, exist_ok=True)

        # ç”Ÿæˆæ—¶é—´æˆ³
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        env_name = self.env_name
        difficulty = self.difficulty

        # å‡†å¤‡æ•°æ® - ä¸QMIXå®Œå…¨ç›¸åŒçš„ç»“æ„
        training_data = {
            'config': self.config,
            'metrics': {
                'episode_rewards': self.episode_rewards,
                'episode_lengths': self.episode_lengths,
                'losses': self.losses,
                'eval_episodes': self.eval_episodes,
                'eval_rewards': self.eval_rewards,
                'epsilon_history': self.epsilon_history
            },
            'environment': {
                'name': env_name,
                'difficulty': difficulty,
                'n_agents': self.env_info['n_agents'],
                'obs_dims': self.env_info['obs_dims'],
                'act_dims': self.env_info['act_dims']
            },
            'timestamp': timestamp,
            'total_episodes': len(self.episode_rewards)
        }

        # ä¿å­˜ä¸ºJSONæ–‡ä»¶ - ä¸QMIXç›¸åŒçš„å‘½åæ ¼å¼
        filename = f"{env_name}_{difficulty}_training_data_{timestamp}.json"
        filepath = os.path.join(self.save_dir, filename)

        # è½¬æ¢numpyç±»å‹ä¸ºPythonåŸç”Ÿç±»å‹ - ä¸QMIXç›¸åŒçš„è½¬æ¢é€»è¾‘
        def convert_numpy(obj):
            if isinstance(obj, np.ndarray):
                return obj.tolist()
            elif isinstance(obj, np.integer):
                return int(obj)
            elif isinstance(obj, np.floating):
                return float(obj)
            elif isinstance(obj, dict):
                return {key: convert_numpy(value) for key, value in obj.items()}
            elif isinstance(obj, list):
                return [convert_numpy(item) for item in obj]
            else:
                return obj

        training_data_converted = convert_numpy(training_data)

        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(training_data_converted, f, indent=2, ensure_ascii=False)

        print(f"ğŸ’¾ è®­ç»ƒæ•°æ®å·²ä¿å­˜: {filepath}")
        return filepath

    def generate_plots(self):
        """ç”Ÿæˆè®­ç»ƒå›¾è¡¨"""
        try:
            from .utils import save_training_plots

            env_name = f"COMA_{self.env_name}_{self.difficulty}"
            save_training_plots(
                episode_rewards=self.episode_rewards,
                episode_lengths=self.episode_lengths,
                losses=self.losses,
                eval_episodes=self.eval_episodes,
                eval_rewards=self.eval_rewards,
                epsilon_values=self.epsilon_history,
                save_dir=self.plot_dir,
                env_name=env_name,
                config=self.config,
                show_plots=False
            )

            logger.info(f"è®­ç»ƒå›¾è¡¨å·²ä¿å­˜åˆ°: {self.plot_dir}")

        except ImportError as e:
            logger.warning(f"æ— æ³•ç”Ÿæˆå›¾è¡¨: {e}")
        except Exception as e:
            logger.error(f"ç”Ÿæˆå›¾è¡¨æ—¶å‘ç”Ÿé”™è¯¯: {e}")