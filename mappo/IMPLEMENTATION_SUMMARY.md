# MAPPO å®ç°æ€»ç»“

## âœ… å®ç°å®Œæˆæƒ…å†µ

å·²æˆåŠŸåœ¨ `mappo/` æ–‡ä»¶å¤¹ä¸­å®ç°å®Œæ•´çš„MAPPOï¼ˆMulti-Agent Proximal Policy Optimizationï¼‰ç®—æ³•ã€‚

### ğŸ“Š é¡¹ç›®æ¦‚è§ˆ

MAPPOæ˜¯åŸºäºPPOçš„å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œé‡‡ç”¨é›†ä¸­å¼è®­ç»ƒåˆ†å¸ƒå¼æ‰§è¡Œï¼ˆCTDEï¼‰æ¡†æ¶ï¼Œç‰¹åˆ«é€‚åˆåä½œå‹å¤šæ™ºèƒ½ä½“ä»»åŠ¡ã€‚

### ğŸ¯ æ ¸å¿ƒç‰¹æ€§

1. **PPOä¼˜åŒ–**: ä½¿ç”¨é‡è¦æ€§é‡‡æ ·æ¯”ç‡è£å‰ªï¼Œç¡®ä¿ç­–ç•¥æ›´æ–°ç¨³å®š
2. **GAEä¼˜åŠ¿ä¼°è®¡**: å¹¿ä¹‰ä¼˜åŠ¿ä¼°è®¡ï¼Œå¹³è¡¡åå·®ä¸æ–¹å·®
3. **é›†ä¸­å¼Critic**: è®­ç»ƒæ—¶ä½¿ç”¨å…¨å±€çŠ¶æ€ï¼Œåˆ†å¸ƒå¼æ‰§è¡Œ
4. **ç†µæ­£åˆ™åŒ–**: è‡ªç„¶çš„æ¢ç´¢æœºåˆ¶ï¼Œä¿ƒè¿›ç­–ç•¥å¤šæ ·æ€§
5. **æ­£äº¤åˆå§‹åŒ–**: æ”¹å–„è®­ç»ƒåˆæœŸçš„æ€§èƒ½

---

## ğŸ“ æ–‡ä»¶ç»“æ„

```
mappo/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py              # åŒ…åˆå§‹åŒ– (2è¡Œ)
â”‚   â”œâ”€â”€ models.py                # ç½‘ç»œæ¨¡å‹ (382è¡Œ)
â”‚   â”œâ”€â”€ algos.py                 # ç®—æ³•æ ¸å¿ƒ (370è¡Œ)
â”‚   â”œâ”€â”€ trainer.py               # è®­ç»ƒå™¨ (269è¡Œ)
â”‚   â”œâ”€â”€ envs.py                  # ç¯å¢ƒåŒ…è£…å™¨ (å¤ç”¨)
â”‚   â”œâ”€â”€ utils.py                 # å·¥å…·å‡½æ•° (å¤ç”¨)
â”‚   â””â”€â”€ pettingzoo_adapter.py    # PettingZooé€‚é…å™¨ (å¤ç”¨)
â”œâ”€â”€ checkpoints/                 # æ£€æŸ¥ç‚¹ç›®å½•
â”œâ”€â”€ plots/                       # å›¾è¡¨ç›®å½•
â”œâ”€â”€ config.yaml                  # é»˜è®¤é…ç½®
â”œâ”€â”€ config_*.yaml               # 9ä¸ªç¯å¢ƒé…ç½®
â”œâ”€â”€ main.py                      # ä¸»è„šæœ¬ (176è¡Œ)
â”œâ”€â”€ README.md                    # ä½¿ç”¨æ–‡æ¡£ (208è¡Œ)
â””â”€â”€ IMPLEMENTATION_SUMMARY.md    # æœ¬æ–‡ä»¶
```

**æ€»ä»£ç é‡**: ~1,400è¡Œï¼ˆä¸å«å¤ç”¨çš„ç¯å¢ƒã€å·¥å…·ç­‰ï¼‰

---

## ğŸ”§ æ ¸å¿ƒå®ç°è¯¦è§£

### 1. ç½‘ç»œæ¶æ„ (`src/models.py`)

#### ActorNetwork (ç­–ç•¥ç½‘ç»œ)
```python
è¾“å…¥: å•ä¸ªæ™ºèƒ½ä½“çš„è§‚æµ‹ (obs_dim)
ç»“æ„: FC(obs_dim, hidden) -> ReLU -> FC(hidden, hidden) -> ReLU -> FC(hidden, action_dim)
è¾“å‡º: åŠ¨ä½œlogits (action_dim)
```

**å…³é”®æ–¹æ³•**:
- `forward()`: å‰å‘ä¼ æ’­ï¼Œè¾“å‡ºåŠ¨ä½œlogits
- `get_action_probs()`: è·å–åŠ¨ä½œæ¦‚ç‡åˆ†å¸ƒ
- `evaluate_actions()`: è¯„ä¼°åŠ¨ä½œï¼Œè¿”å›å¯¹æ•°æ¦‚ç‡å’Œç†µ

#### CriticNetwork (ä»·å€¼ç½‘ç»œ)
```python
è¾“å…¥: å…¨å±€çŠ¶æ€ (æ‰€æœ‰æ™ºèƒ½ä½“çš„è§‚æµ‹æ‹¼æ¥)
ç»“æ„: FC(state_dim, hidden) -> ReLU -> FC(hidden, hidden) -> ReLU -> FC(hidden, 1)
è¾“å‡º: çŠ¶æ€ä»·å€¼ V(s)
```

**ç‰¹ç‚¹**: é›†ä¸­å¼Criticï¼Œä½¿ç”¨å…¨å±€ä¿¡æ¯è¯„ä¼°çŠ¶æ€ä»·å€¼

#### åˆå§‹åŒ–ç­–ç•¥
- **æ­£äº¤åˆå§‹åŒ–**: `use_orthogonal_init=True`æ—¶ä½¿ç”¨ï¼Œæ”¹å–„è®­ç»ƒåˆæœŸæ€§èƒ½
- **Xavieråˆå§‹åŒ–**: é»˜è®¤åˆå§‹åŒ–æ–¹å¼

### 2. ç®—æ³•æ ¸å¿ƒ (`src/algos.py`)

#### RolloutBuffer (è½¨è¿¹ç¼“å†²åŒº)
å­˜å‚¨ä¸€ä¸ªepisodeçš„å®Œæ•´è½¨è¿¹:
- `obs`: è§‚æµ‹åºåˆ—
- `states`: å…¨å±€çŠ¶æ€åºåˆ—
- `actions`: åŠ¨ä½œåºåˆ—
- `action_log_probs`: åŠ¨ä½œå¯¹æ•°æ¦‚ç‡
- `rewards`: å¥–åŠ±åºåˆ—
- `values`: ä»·å€¼ä¼°è®¡
- `dones`: ç»ˆæ­¢æ ‡å¿—

#### PPOæ›´æ–°æµç¨‹

```python
for epoch in range(ppo_epochs):
    for agent in agents:
        # 1. è¯„ä¼°åŠ¨ä½œ
        new_log_probs, entropy = evaluate_actions(obs, actions)
        
        # 2. è®¡ç®—é‡è¦æ€§é‡‡æ ·æ¯”ç‡
        ratio = exp(new_log_probs - old_log_probs)
        
        # 3. PPOè£å‰ª
        surr1 = ratio * advantages
        surr2 = clip(ratio, 1-Îµ, 1+Îµ) * advantages
        actor_loss = -min(surr1, surr2).mean()
        
        # 4. æ›´æ–°Actor
        update_actor(actor_loss - entropy_coef * entropy)
    
    # 5. æ›´æ–°Criticï¼ˆå…±äº«ï¼‰
    critic_loss = MSE(V(s), returns)
    update_critic(value_loss_coef * critic_loss)
```

#### GAEè®¡ç®—

```python
def compute_gae(rewards, values, dones, next_value):
    advantages = []
    gae = 0
    
    for t in reversed(range(T)):
        if t == T-1:
            next_val = next_value
        else:
            next_val = values[t+1]
        
        # TDè¯¯å·®
        delta = rewards[t] + gamma * next_val * (1 - dones[t]) - values[t]
        
        # GAEé€’æ¨
        gae = delta + gamma * lambda * (1 - dones[t]) * gae
        advantages.insert(0, gae)
    
    returns = advantages + values
    return returns, advantages
```

### 3. è®­ç»ƒæµç¨‹ (`src/trainer.py`)

#### Episodeæ”¶é›†
```python
for step in range(episode_length):
    1. é€‰æ‹©åŠ¨ä½œï¼ˆä»ç­–ç•¥é‡‡æ ·ï¼‰
    2. è·å–ä»·å€¼ä¼°è®¡ V(s)
    3. ç¯å¢ƒäº¤äº’
    4. å­˜å‚¨åˆ°buffer: (obs, state, action, log_prob, reward, value, done)
```

#### è®­ç»ƒæ›´æ–°
```python
1. è®¡ç®—GAEä¼˜åŠ¿
2. å½’ä¸€åŒ–ä¼˜åŠ¿
3. PPOå¤šè½®æ›´æ–°
4. æ¸…ç©ºbuffer
```

**å…³é”®å·®å¼‚**:
- **On-policy**: æ¯ä¸ªepisodeåç«‹å³æ›´æ–°å¹¶æ¸…ç©ºbuffer
- **å¤šè½®æ›´æ–°**: å¯¹åŒä¸€æ‰¹æ•°æ®æ›´æ–°å¤šæ¬¡ï¼ˆppo_epochsï¼‰

---

## âš™ï¸ é…ç½®å‚æ•°

### PPOç‰¹æœ‰å‚æ•°

| å‚æ•° | é»˜è®¤å€¼ | è¯´æ˜ |
|------|--------|------|
| `gae_lambda` | 0.95 | GAEçš„Î»å‚æ•°ï¼Œæ§åˆ¶åå·®-æ–¹å·®æƒè¡¡ |
| `clip_param` | 0.2 | PPOè£å‰ªå‚æ•°Îµï¼Œé™åˆ¶ç­–ç•¥æ›´æ–°å¹…åº¦ |
| `value_loss_coef` | 0.5 | ä»·å€¼å‡½æ•°æŸå¤±ç³»æ•°c1 |
| `entropy_coef` | 0.01 | ç†µç³»æ•°c2ï¼Œä¿ƒè¿›æ¢ç´¢ |
| `ppo_epochs` | 4 | PPOæ›´æ–°è½®æ•° |
| `num_mini_batch` | 1 | mini-batchæ•°é‡ |

### å­¦ä¹ ç‡
- `actor_lr: 0.0003` - ç›¸å¯¹è¾ƒä½ï¼Œç¡®ä¿ç¨³å®šæ€§
- `critic_lr: 0.001` - ç¨é«˜ï¼ŒåŠ å¿«ä»·å€¼å‡½æ•°å­¦ä¹ 

### è®­ç»ƒå‚æ•°
- `episode_length: 200` - æ¯ä¸ªepisodeæœ€å¤§æ­¥æ•°
- `buffer_size: 200` - å›æ”¾ç¼“å†²åŒºå¤§å°
- **æ³¨æ„**: MAPPOçš„bufferæ˜¯episode bufferï¼Œè€Œéreplay buffer

---

## ğŸ†š ç®—æ³•å¯¹æ¯”

### MAPPO vs QMIX vs MADDPG

| ç»´åº¦ | MAPPO | QMIX | MADDPG |
|------|-------|------|--------|
| **ç†è®ºåŸºç¡€** | PPO | Q-learning + Value Decomposition | DDPG |
| **On/Off-policy** | On-policy | Off-policy | Off-policy |
| **æ ·æœ¬æ•ˆç‡** | ä½ | ä¸­ | é«˜ |
| **è®­ç»ƒç¨³å®šæ€§** | é«˜ | ä¸­ | ä¸­ |
| **è¶…å‚æ•æ„Ÿåº¦** | ä½ | ä¸­ | é«˜ |
| **æ¢ç´¢æœºåˆ¶** | ç­–ç•¥ç†µ | Îµ-greedy | å™ªå£°æ³¨å…¥ |
| **Criticç»“æ„** | Vå‡½æ•°(å…±äº«) | Mixing Network | Qå‡½æ•°(æ¯ä¸ªæ™ºèƒ½ä½“) |
| **æ›´æ–°æ–¹å¼** | Episode batch | Experience replay | Experience replay |
| **é€‚ç”¨ä»»åŠ¡** | åä½œ | åä½œ | åä½œ/ç«äº‰/æ··åˆ |

### æ€§èƒ½ç‰¹ç‚¹

**MAPPOä¼˜åŠ¿**:
- è®­ç»ƒæœ€ç¨³å®šï¼Œå‡ ä¹ä¸éœ€è¦è°ƒå‚
- é€‚åˆé•¿æœŸåä½œä»»åŠ¡
- è‡ªç„¶çš„æ¢ç´¢æœºåˆ¶
- æ”¶æ•›æ€§èƒ½é€šå¸¸è¾ƒå¥½

**MAPPOåŠ£åŠ¿**:
- æ ·æœ¬æ•ˆç‡ä½ï¼ˆéœ€è¦æ›´å¤šäº¤äº’ï¼‰
- è®­ç»ƒæ—¶é—´è¾ƒé•¿
- å†…å­˜å ç”¨è¾ƒå¤§ï¼ˆå­˜å‚¨å®Œæ•´è½¨è¿¹ï¼‰

---

## ğŸ“Š å…³é”®æŠ€æœ¯ç»†èŠ‚

### 1. é‡è¦æ€§é‡‡æ ·è£å‰ª

```python
ratio = exp(log_Ï€_new(a|s) - log_Ï€_old(a|s))
clipped_ratio = clip(ratio, 1-Îµ, 1+Îµ)

# é€‰æ‹©è¾ƒå°å€¼ï¼ˆä¿å®ˆæ›´æ–°ï¼‰
L_CLIP = -min(ratio * A, clipped_ratio * A)
```

**ç›®çš„**: é™åˆ¶ç­–ç•¥æ›´æ–°å¹…åº¦ï¼Œé˜²æ­¢æ€§èƒ½å´©æºƒ

### 2. ä¼˜åŠ¿å½’ä¸€åŒ–

```python
advantages = (advantages - mean(advantages)) / (std(advantages) + 1e-8)
```

**ç›®çš„**: ç¨³å®šè®­ç»ƒï¼Œå‡å°‘ä¸åŒepisodeé—´çš„å°ºåº¦å·®å¼‚

### 3. æ¢¯åº¦è£å‰ª

```python
torch.nn.utils.clip_grad_norm_(parameters, max_grad_norm)
```

**ç›®çš„**: é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸

### 4. æ­£äº¤åˆå§‹åŒ–

```python
nn.init.orthogonal_(weight, gain=0.01)  # Actor
nn.init.orthogonal_(weight, gain=1.0)   # Critic
```

**ç›®çš„**: æ”¹å–„è®­ç»ƒåˆæœŸçš„æ¢¯åº¦æµåŠ¨

---

## ğŸ¯ ä½¿ç”¨æŒ‡å—

### åŸºæœ¬è®­ç»ƒ
```bash
cd mappo
conda activate pettingzoo
python main.py
```

### ä¸‰ç®—æ³•å¯¹æ¯”å®éªŒ
```bash
# ç»ˆç«¯1: MAPPO
cd mappo
python main.py --env CM --difficulty hard --plots --plot-dir ../results/mappo

# ç»ˆç«¯2: QMIX
cd ../qmix
python main.py --env CM --difficulty hard --plots --plot-dir ../results/qmix

# ç»ˆç«¯3: MADDPG
cd ../maddpg
python main.py --env CM --difficulty hard --plots --plot-dir ../results/maddpg
```

### è°ƒå‚å»ºè®®

**æé«˜æ ·æœ¬æ•ˆç‡**:
- å¢åŠ `ppo_epochs`ï¼ˆå¦‚6-8ï¼‰
- å¢åŠ `episode_length`

**æé«˜ç¨³å®šæ€§**:
- é™ä½`clip_param`ï¼ˆå¦‚0.1ï¼‰
- é™ä½å­¦ä¹ ç‡

**ä¿ƒè¿›æ¢ç´¢**:
- å¢åŠ `entropy_coef`ï¼ˆå¦‚0.02-0.05ï¼‰

---

## ğŸ“ˆ é¢„æœŸæ€§èƒ½

### æ”¶æ•›é€Ÿåº¦
- **åˆæœŸ**: è¾ƒæ…¢ï¼ˆéœ€è¦æ”¶é›†è¶³å¤Ÿæ•°æ®ï¼‰
- **ä¸­æœŸ**: ç¨³å®šæå‡
- **åæœŸ**: å¹³ç¨³æ”¶æ•›

### ä¸å…¶ä»–ç®—æ³•å¯¹æ¯”
- **vs QMIX**: è®­ç»ƒæ›´ç¨³å®šï¼Œæœ€ç»ˆæ€§èƒ½é€šå¸¸æ›´å¥½ï¼Œä½†éœ€è¦æ›´å¤šepisodes
- **vs MADDPG**: æ ·æœ¬æ•ˆç‡ä½ï¼Œä½†è®­ç»ƒç¨³å®šæ€§é«˜ï¼Œé€‚åˆä¸æ˜“è°ƒå‚çš„åœºæ™¯

---

## âœ¨ å®ç°äº®ç‚¹

1. **å®Œæ•´çš„PPOå®ç°**: åŒ…æ‹¬GAEã€è£å‰ªã€ç†µæ­£åˆ™åŒ–ç­‰æ‰€æœ‰å…³é”®ç»„ä»¶
2. **çµæ´»çš„ç½‘ç»œåˆå§‹åŒ–**: æ”¯æŒæ­£äº¤åˆå§‹åŒ–å’ŒXavieråˆå§‹åŒ–
3. **å¼‚æ„è§‚æµ‹æ”¯æŒ**: å¯å¤„ç†ä¸åŒæ™ºèƒ½ä½“è§‚æµ‹ç»´åº¦ä¸åŒçš„æƒ…å†µ
4. **æ¨¡å—åŒ–è®¾è®¡**: æ˜“äºæ‰©å±•å’Œä¿®æ”¹
5. **å®Œå–„çš„æ–‡æ¡£**: ä»£ç æ³¨é‡Šè¯¦ç»†ï¼ŒREADMEæ¸…æ™°

---

## ğŸ”¬ æŠ€æœ¯åˆ›æ–°ç‚¹

### 1. é›†ä¸­å¼Critic + åˆ†å¸ƒå¼Actor
- Criticä½¿ç”¨å…¨å±€çŠ¶æ€ â†’ å‡†ç¡®çš„ä»·å€¼ä¼°è®¡
- Actorä½¿ç”¨å±€éƒ¨è§‚æµ‹ â†’ åˆ†å¸ƒå¼æ‰§è¡Œ

### 2. PPOçš„ç¨³å®šæ€§
- è£å‰ªæœºåˆ¶ â†’ é¿å…å¤§å¹…åº¦ç­–ç•¥æ›´æ–°
- å¤šè½®æ›´æ–° â†’ å……åˆ†åˆ©ç”¨æ•°æ®
- ç†µæ­£åˆ™åŒ– â†’ è‡ªç„¶çš„æ¢ç´¢

### 3. GAEä¼˜åŠ¿ä¼°è®¡
- Î»å‚æ•°æƒè¡¡ â†’ åå·®ä¸æ–¹å·®
- é€’æ¨è®¡ç®— â†’ è®¡ç®—é«˜æ•ˆ

---

## ğŸ“š å‚è€ƒå®ç°

æœ¬å®ç°å‚è€ƒ:
1. `maddpg/` - ä»£ç ç»“æ„
2. `qmix/` - ç¯å¢ƒæ¥å£
3. PPOè®ºæ–‡åŸç†
4. MAPPOè®ºæ–‡è®¾è®¡

---

## âœ… æ€»ç»“

**å®ç°å®Œæ•´åº¦**: 100%
- âœ… æ‰€æœ‰æ ¸å¿ƒç»„ä»¶å·²å®ç°
- âœ… æ‰€æœ‰é…ç½®æ–‡ä»¶å·²åˆ›å»º  
- âœ… å®Œæ•´æ–‡æ¡£å·²ç¼–å†™

**ä»£ç è´¨é‡**:
- âœ… å®Œæ•´çš„æ³¨é‡Šå’Œæ–‡æ¡£
- âœ… å¼‚å¸¸å¤„ç†å’Œå‚æ•°éªŒè¯
- âœ… æ”¯æŒå¼‚æ„è§‚æµ‹
- âœ… æ¨¡å—åŒ–è®¾è®¡

**å¯ç”¨æ€§**:
- âœ… ç®€å•çš„å‘½ä»¤è¡Œæ¥å£
- âœ… ä¸QMIX/MADDPGä¸€è‡´çš„é…ç½®
- âœ… æ”¯æŒæ‰€æœ‰ç›¸åŒç¯å¢ƒ
- âœ… å¯ç›´æ¥ç”¨äºç®—æ³•å¯¹æ¯”

**MAPPOç®—æ³•å·²æˆåŠŸå®ç°ï¼Œå¯ç”¨äºå¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ç ”ç©¶ï¼** ğŸ‰
