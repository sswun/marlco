"""
IQL ç®—æ³•æµ‹è¯•è„šæœ¬
"""
import torch
import numpy as np
from src.models import IQLNetworks, AgentNetwork
from src.algos import IQL
from src.buffer import ReplayBuffer
from src.utils import get_device, set_seed


def test_agent_network():
    """æµ‹è¯•AgentNetwork"""
    print("ğŸ§ª æµ‹è¯•AgentNetwork...")

    device = get_device()

    # åˆ›å»ºç½‘ç»œ
    net = AgentNetwork(obs_dim=10, action_dim=5, hidden_dim=32).to(device)

    # æµ‹è¯•å‰å‘ä¼ æ’­
    obs = torch.randn(2, 10).to(device)  # batch_size=2
    q_values = net(obs)

    assert q_values.shape == (2, 5), f"æœŸæœ›å½¢çŠ¶(2, 5)ï¼Œå®é™…{q_values.shape}"
    print("   âœ… å‰å‘ä¼ æ’­æµ‹è¯•é€šè¿‡")

    # æµ‹è¯•åŠ¨ä½œé€‰æ‹©
    actions = net.act(obs, epsilon=0.0)
    assert actions.shape == (2,), f"æœŸæœ›å½¢çŠ¶(2,)ï¼Œå®é™…{actions.shape}"
    assert torch.all(actions >= 0) and torch.all(actions < 5), "åŠ¨ä½œå€¼è¶…å‡ºèŒƒå›´"
    print("   âœ… åŠ¨ä½œé€‰æ‹©æµ‹è¯•é€šè¿‡")

    print("âœ… AgentNetworkæµ‹è¯•å®Œæˆ\n")


def test_iql_networks():
    """æµ‹è¯•IQLNetworks"""
    print("ğŸ§ª æµ‹è¯•IQLNetworks...")

    device = get_device()

    # æ¨¡æ‹Ÿç¯å¢ƒä¿¡æ¯
    env_info = {
        'n_agents': 3,
        'obs_dims': [10, 12, 10],  # å¼‚æ„è§‚æµ‹ç»´åº¦
        'act_dims': [5, 5, 5]
    }

    config = {
        'model': {
            'hidden_dim': 32
        }
    }

    # åˆ›å»ºç½‘ç»œ
    networks = IQLNetworks(env_info, config, device)

    assert networks.n_agents == 3
    assert len(networks.agent_networks) == 3
    assert len(networks.target_agent_networks) == 3
    print("   âœ… ç½‘ç»œåˆ›å»ºæµ‹è¯•é€šè¿‡")

    # æµ‹è¯•å‚æ•°è·å–
    params = networks.get_all_parameters()
    assert len(params) > 0, "å‚æ•°åˆ—è¡¨ä¸ºç©º"
    print("   âœ… å‚æ•°è·å–æµ‹è¯•é€šè¿‡")

    # æµ‹è¯•ç›®æ ‡ç½‘ç»œæ›´æ–°
    networks.hard_update_target_networks()
    networks.soft_update_target_networks(tau=0.01)
    print("   âœ… ç›®æ ‡ç½‘ç»œæ›´æ–°æµ‹è¯•é€šè¿‡")

    print("âœ… IQLNetworksæµ‹è¯•å®Œæˆ\n")


def test_replay_buffer():
    """æµ‹è¯•ReplayBuffer"""
    print("ğŸ§ª æµ‹è¯•ReplayBuffer...")

    device = get_device()

    # åˆ›å»ºç¼“å†²åŒº
    buffer = ReplayBuffer(
        capacity=1000,
        n_agents=3,
        obs_dim=10,
        state_dim=1,  # IQLä¸­ä¸ä½¿ç”¨å…¨å±€çŠ¶æ€
        device=device
    )

    # æµ‹è¯•æ·»åŠ ç»éªŒ
    for _ in range(10):
        obs = {f'agent_{i}': np.random.randn(10) for i in range(3)}
        actions = {f'agent_{i}': np.random.randint(0, 5) for i in range(3)}
        rewards = {f'agent_{i}': np.random.randn() for i in range(3)}
        next_obs = {f'agent_{i}': np.random.randn(10) for i in range(3)}
        dones = {f'agent_{i}': False for i in range(3)}

        buffer.push(obs, actions, rewards, next_obs, dones)

    assert len(buffer) == 10, f"ç¼“å†²åŒºå¤§å°é”™è¯¯ï¼ŒæœŸæœ›10ï¼Œå®é™…{len(buffer)}"
    print("   âœ… ç»éªŒæ·»åŠ æµ‹è¯•é€šè¿‡")

    # æµ‹è¯•é‡‡æ ·
    batch = buffer.sample(batch_size=5)

    required_keys = ['obs', 'actions', 'rewards', 'next_obs', 'dones', 'global_state', 'next_global_state']
    for key in required_keys:
        assert key in batch, f"æ‰¹æ¬¡æ•°æ®ç¼ºå°‘é”®: {key}"

    assert batch['obs'].shape[0] == 5, "æ‰¹æ¬¡å¤§å°é”™è¯¯"
    assert batch['obs'].shape[1] == 3, "æ™ºèƒ½ä½“æ•°é‡é”™è¯¯"
    print("   âœ… æ‰¹é‡é‡‡æ ·æµ‹è¯•é€šè¿‡")

    print("âœ… ReplayBufferæµ‹è¯•å®Œæˆ\n")


def test_iql_algorithm():
    """æµ‹è¯•IQLç®—æ³•"""
    print("ğŸ§ª æµ‹è¯•IQLç®—æ³•...")

    device = get_device()

    # ç¯å¢ƒä¿¡æ¯å’Œé…ç½®
    env_info = {
        'n_agents': 3,
        'obs_dims': [10, 10, 10],  # åŒæ„è§‚æµ‹ç»´åº¦
        'act_dims': [5, 5, 5]
    }

    config = {
        'algorithm': {
            'gamma': 0.99,
            'learning_rate': 0.001,
            'tau': 0.005,
            'target_update_interval': 10,
            'max_grad_norm': 10.0
        },
        'model': {
            'hidden_dim': 32
        }
    }

    # åˆ›å»ºç½‘ç»œå’Œç®—æ³•
    networks = IQLNetworks(env_info, config, device)
    algorithm = IQL(networks, config, device)

    # æµ‹è¯•åŠ¨ä½œé€‰æ‹©
    obs = {
        'agent_0': torch.randn(10).to(device),
        'agent_1': torch.randn(10).to(device),
        'agent_2': torch.randn(10).to(device)
    }

    actions = algorithm.select_actions(obs, epsilon=0.0)
    assert len(actions) == 3, "åŠ¨ä½œæ•°é‡é”™è¯¯"
    for action in actions.values():
        assert 0 <= action < 5, "åŠ¨ä½œå€¼è¶…å‡ºèŒƒå›´"
    print("   âœ… åŠ¨ä½œé€‰æ‹©æµ‹è¯•é€šè¿‡")

    # åˆ›å»ºæµ‹è¯•æ‰¹æ¬¡
    batch_size = 4
    batch = {
        'obs': torch.randn(batch_size, 3, 10).to(device),
        'actions': torch.randint(0, 5, (batch_size, 3)).to(device),
        'rewards': torch.randn(batch_size, 3).to(device),
        'next_obs': torch.randn(batch_size, 3, 10).to(device),
        'dones': torch.zeros(batch_size, 3, dtype=torch.bool).to(device),
        'global_state': torch.randn(batch_size, 1).to(device),
        'next_global_state': torch.randn(batch_size, 1).to(device)
    }

    # æµ‹è¯•æŸå¤±è®¡ç®—
    loss = algorithm.compute_loss(batch)
    assert loss.requires_grad, "æŸå¤±éœ€è¦æ¢¯åº¦"
    print("   âœ… æŸå¤±è®¡ç®—æµ‹è¯•é€šè¿‡")

    # æµ‹è¯•ç®—æ³•æ›´æ–°
    loss_info = algorithm.update(batch)
    assert 'loss' in loss_info, "æ›´æ–°ç»“æœç¼ºå°‘æŸå¤±ä¿¡æ¯"
    assert 'grad_norm' in loss_info, "æ›´æ–°ç»“æœç¼ºå°‘æ¢¯åº¦èŒƒæ•°ä¿¡æ¯"
    print("   âœ… ç®—æ³•æ›´æ–°æµ‹è¯•é€šè¿‡")

    print("âœ… IQLç®—æ³•æµ‹è¯•å®Œæˆ\n")


def test_integration():
    """é›†æˆæµ‹è¯•"""
    print("ğŸ§ª é›†æˆæµ‹è¯•...")

    device = get_device()

    # ç¯å¢ƒä¿¡æ¯å’Œé…ç½®
    env_info = {
        'n_agents': 2,
        'obs_dims': [8, 8],
        'act_dims': [4, 4]
    }

    config = {
        'algorithm': {
            'gamma': 0.95,
            'learning_rate': 0.01,
            'tau': 0.01,
            'target_update_interval': 5,
            'max_grad_norm': 5.0
        },
        'model': {
            'hidden_dim': 16
        }
    }

    # åˆ›å»ºç»„ä»¶
    networks = IQLNetworks(env_info, config, device)
    algorithm = IQL(networks, config, device)
    buffer = ReplayBuffer(
        capacity=100,
        n_agents=2,
        obs_dim=8,
        state_dim=1,
        device=device
    )

    # æ¨¡æ‹Ÿè®­ç»ƒè¿‡ç¨‹
    print("   ğŸ”„ æ¨¡æ‹Ÿè®­ç»ƒè¿‡ç¨‹...")

    for episode in range(10):
        obs = {f'agent_{i}': np.random.randn(8) for i in range(2)}

        for step in range(5):
            # é€‰æ‹©åŠ¨ä½œ
            obs_tensor = {k: torch.FloatTensor(v).to(device) for k, v in obs.items()}
            actions = algorithm.select_actions(obs_tensor, epsilon=0.1)

            # æ¨¡æ‹Ÿç¯å¢ƒäº¤äº’
            next_obs = {f'agent_{i}': np.random.randn(8) for i in range(2)}
            rewards = {f'agent_{i}': np.random.randn() for i in range(2)}
            dones = {f'agent_{i}': step == 4 for i in range(2)}  # æœ€åä¸€æ­¥ç»“æŸ

            # å­˜å‚¨ç»éªŒ
            buffer.push(obs, actions, rewards, next_obs, dones)
            obs = next_obs

        # è®­ç»ƒ
        if len(buffer) >= 4:
            batch = buffer.sample(batch_size=4)
            loss_info = algorithm.update(batch)

            if episode % 3 == 0:
                print(f"      Episode {episode}: Loss = {loss_info['loss']:.4f}")

    print("   âœ… æ¨¡æ‹Ÿè®­ç»ƒå®Œæˆ")
    print("âœ… é›†æˆæµ‹è¯•å®Œæˆ\n")


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ å¼€å§‹IQLç®—æ³•æµ‹è¯•\n")

    # è®¾ç½®éšæœºç§å­
    set_seed(42)

    try:
        test_agent_network()
        test_iql_networks()
        test_replay_buffer()
        test_iql_algorithm()
        test_integration()

        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼IQLç®—æ³•å®ç°æ­£ç¡®ã€‚")

    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

    return True


if __name__ == "__main__":
    main()