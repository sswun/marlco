"""
MADDPG è®­ç»ƒå™¨
"""
import time
import numpy as np
import torch
from typing import Dict, Any, List, Tuple
from .envs import EnvWrapper
from .models import MADDPGNetworks
from .algos import MADDPG
from .buffer import ReplayBuffer
from .utils import get_device, to_tensor


class MADDPGTrainer:
    """MADDPGè®­ç»ƒå™¨"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.device = get_device()
        
        # åˆ›å»ºç¯å¢ƒ
        self.env = EnvWrapper(config)
        self.env_info = self.env.get_env_info()
        
        print(f"ğŸŒ Environment: {config['env']['name']}")
        print(f"   Agents: {self.env_info['n_agents']}")
        print(f"   Obs dims: {self.env_info['obs_dims']}")
        print(f"   Action dims: {self.env_info['act_dims']}")
        print(f"   Device: {self.device}")
        
        # åˆ›å»ºç¼“å†²åŒº
        self.buffer = ReplayBuffer(
            capacity=config['training']['buffer_size'],
            n_agents=self.env_info['n_agents'],
            obs_dim=self.env_info['obs_dims'][0],
            state_dim=0,  # MADDPGä¸éœ€è¦å…¨å±€çŠ¶æ€
            device=self.device
        )

        # åˆ›å»ºç½‘ç»œ
        self.networks = MADDPGNetworks(self.env_info, config, self.device)

        # åˆ›å»ºç®—æ³•
        self.algorithm = MADDPG(self.networks, config, self.device)
        
        # è®­ç»ƒå‚æ•°
        self.total_episodes = config['training']['total_episodes']
        self.batch_size = config['training']['batch_size']
        self.warmup_episodes = config['training']['warmup_episodes']
        self.eval_interval = config['training']['eval_interval']
        self.save_interval = config['training']['save_interval']
        
        # æ¢ç´¢å‚æ•°
        self.noise_scale = config['exploration']['noise_scale']
        self.noise_decay = config['exploration']['noise_decay']
        self.noise_min = config['exploration']['noise_min']
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.episode_rewards = []
        self.episode_lengths = []
        self.losses = []
        self.eval_episodes = []
        self.eval_rewards = []
        self.noise_history = []
        
    def train(self) -> List[float]:
        """ä¸»è®­ç»ƒå¾ªç¯"""
        print(f"\nğŸš€ å¼€å§‹MADDPGè®­ç»ƒ...")
        print(f"   æ€»episodes: {self.total_episodes}")
        print(f"   é¢„çƒ­episodes: {self.warmup_episodes}")
        print(f"   æ‰¹å¤§å°: {self.batch_size}")
        
        start_time = time.time()
        
        for episode in range(self.total_episodes):
            # æ”¶é›†ä¸€ä¸ªepisodeçš„ç»éªŒ
            episode_reward, episode_length = self.collect_episode()
            self.episode_rewards.append(episode_reward)
            self.episode_lengths.append(episode_length)
            
            # è®­ç»ƒç½‘ç»œ
            if len(self.buffer) >= self.batch_size and episode >= self.warmup_episodes:
                loss_info = self.train_step()
                self.losses.append(loss_info['total_loss'])
            
            # æ›´æ–°å™ªå£°å°ºåº¦
            self.noise_scale = max(self.noise_min, self.noise_scale * self.noise_decay)
            self.noise_history.append(self.noise_scale)

            # æ—¥å¿—å’Œè¯„ä¼°
            if episode % 100 == 0:
                avg_reward = np.mean(self.episode_rewards[-100:])
                avg_length = np.mean(self.episode_lengths[-100:])
                print(f"Episode {episode:6d} | Avg Reward: {avg_reward:6.2f} | "
                      f"Avg Length: {avg_length:4.1f} | Noise: {self.noise_scale:.3f} | "
                      f"Buffer: {len(self.buffer):5d}")

            # è¯„ä¼°
            if episode % self.eval_interval == 0 and episode > 0:
                eval_reward = self.evaluate()
                self.eval_episodes.append(episode)
                self.eval_rewards.append(eval_reward)
                print(f"ğŸ¯ Evaluation at episode {episode}: {eval_reward:.2f}")
            
            # ä¿å­˜æ¨¡å‹
            if episode % self.save_interval == 0 and episode > 0:
                self.save_checkpoint(episode)
        
        total_time = time.time() - start_time
        print(f"\nâœ… è®­ç»ƒå®Œæˆ! æ€»æ—¶é—´: {total_time:.1f}s")
        
        return self.episode_rewards
    
    def collect_episode(self) -> Tuple[float, int]:
        """æ”¶é›†ä¸€ä¸ªepisodeçš„ç»éªŒ"""
        obs, _ = self.env.reset()

        episode_reward = 0
        step_count = 0
        
        while True:
            # è½¬æ¢è§‚æµ‹ä¸ºtensor
            obs_tensor = {}
            for agent_id, agent_obs in obs.items():
                obs_tensor[agent_id] = to_tensor(agent_obs, device=self.device)
            
            # è·å–å¯ç”¨åŠ¨ä½œï¼ˆå¦‚æœç¯å¢ƒæ”¯æŒï¼‰
            avail_actions = None
            if hasattr(self.env, 'get_avail_actions'):
                avail_actions = {}
                for agent_id in obs.keys():
                    avail_actions[agent_id] = self.env.get_avail_actions(agent_id)
            
            # é€‰æ‹©åŠ¨ä½œï¼ˆå¸¦æ¢ç´¢å™ªå£°ï¼‰
            actions = self.algorithm.select_actions(obs_tensor, self.noise_scale, avail_actions)
            
            # ç¯å¢ƒäº¤äº’
            next_obs, rewards, dones, infos = self.env.step(actions)
            
            # å­˜å‚¨ç»éªŒï¼ˆMADDPGä¸éœ€è¦å…¨å±€çŠ¶æ€ï¼‰
            self.buffer.push(
                obs, actions, rewards, next_obs, dones,
                np.zeros(1), np.zeros(1)  # ä¼ å…¥ç©ºçš„å…¨å±€çŠ¶æ€
            )
            
            # æ›´æ–°çŠ¶æ€
            obs = next_obs
            episode_reward += sum(rewards.values())
            step_count += 1
            
            # æ£€æŸ¥ç»“æŸæ¡ä»¶
            if all(dones.values()) or step_count >= 200:  # æœ€å¤§æ­¥æ•°é™åˆ¶
                break
        
        return episode_reward, step_count
    
    def train_step(self) -> Dict[str, float]:
        """æ‰§è¡Œä¸€æ­¥è®­ç»ƒ"""
        batch = self.buffer.sample(self.batch_size)
        loss_info = self.algorithm.update(batch)
        return loss_info
    
    def evaluate(self, num_episodes: int = 10) -> float:
        """è¯„ä¼°æ€§èƒ½"""
        total_rewards = []
        
        for episode in range(num_episodes):
            obs, _ = self.env.reset()
            episode_reward = 0
            step_count = 0
            
            while True:
                # è½¬æ¢è§‚æµ‹ä¸ºtensor
                obs_tensor = {}
                for agent_id, agent_obs in obs.items():
                    obs_tensor[agent_id] = to_tensor(agent_obs, device=self.device)
                
                # è·å–å¯ç”¨åŠ¨ä½œï¼ˆå¦‚æœç¯å¢ƒæ”¯æŒï¼‰
                avail_actions = None
                if hasattr(self.env, 'get_avail_actions'):
                    avail_actions = {}
                    for agent_id in obs.keys():
                        avail_actions[agent_id] = self.env.get_avail_actions(agent_id)
                
                # è´ªå©ªç­–ç•¥ï¼ˆæ— æ¢ç´¢ï¼‰
                actions = self.algorithm.select_actions(obs_tensor, noise_scale=0.0, avail_actions=avail_actions)
                
                # ç¯å¢ƒäº¤äº’
                next_obs, rewards, dones, _ = self.env.step(actions)
                
                obs = next_obs
                episode_reward += sum(rewards.values())
                step_count += 1
                
                if all(dones.values()) or step_count >= 200:
                    break
            
            total_rewards.append(episode_reward)
        
        return float(np.mean(total_rewards))
    
    def save_checkpoint(self, episode: int):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint_path = f"checkpoints/maddpg_episode_{episode}.pt"
        self.algorithm.save(checkpoint_path)
        print(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜: {checkpoint_path}")
    
    def save_training_data(self, save_dir: str = "checkpoints"):
        """
        ä¿å­˜è®­ç»ƒæ•°æ®
        
        Args:
            save_dir: ä¿å­˜ç›®å½•
        """
        import os
        import json
        from datetime import datetime
        
        os.makedirs(save_dir, exist_ok=True)
        
        # ç”Ÿæˆæ—¶é—´æˆ³
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        env_name = self.config['env']['name']
        difficulty = self.config['env']['difficulty']
        
        # å‡†å¤‡æ•°æ®
        training_data = {
            'config': self.config,
            'metrics': {
                'episode_rewards': self.episode_rewards,
                'episode_lengths': self.episode_lengths,
                'losses': self.losses,
                'eval_episodes': self.eval_episodes,
                'eval_rewards': self.eval_rewards,
                'noise_history': self.noise_history
            },
            'environment': {
                'name': env_name,
                'difficulty': difficulty,
                'n_agents': self.env_info['n_agents'],
                'obs_dims': self.env_info['obs_dims'],
                'act_dims': self.env_info['act_dims']
            },
            'timestamp': timestamp,
            'total_episodes': len(self.episode_rewards)
        }
        
        # ä¿å­˜ä¸ºJSONæ–‡ä»¶
        filename = f"{env_name}_{difficulty}_training_data_{timestamp}.json"
        filepath = os.path.join(save_dir, filename)
        
        # è½¬æ¢numpyç±»å‹ä¸ºPythonåŸç”Ÿç±»å‹
        def convert_numpy(obj):
            if isinstance(obj, np.ndarray):
                return obj.tolist()
            elif isinstance(obj, np.integer):
                return int(obj)
            elif isinstance(obj, np.floating):
                return float(obj)
            elif isinstance(obj, dict):
                return {key: convert_numpy(value) for key, value in obj.items()}
            elif isinstance(obj, list):
                return [convert_numpy(item) for item in obj]
            else:
                return obj

        training_data_converted = convert_numpy(training_data)

        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(training_data_converted, f, indent=2, ensure_ascii=False)

        print(f"ğŸ’¾ è®­ç»ƒæ•°æ®å·²ä¿å­˜: {filepath}")
        return filepath
    
    def get_training_metrics(self):
        """Get all training metrics for plotting"""
        return {
            'episode_rewards': self.episode_rewards,
            'episode_lengths': self.episode_lengths,
            'losses': self.losses,
            'eval_episodes': self.eval_episodes,
            'eval_rewards': self.eval_rewards,
            'epsilon_history': self.noise_history  # ä½¿ç”¨noise_historyæ›¿ä»£epsilon
        }

    def close(self):
        """å…³é—­ç¯å¢ƒ"""
        self.env.close()
