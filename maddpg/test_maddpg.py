"""
MADDPG å¿«é€Ÿæµ‹è¯•è„šæœ¬
ç”¨äºéªŒè¯å®ç°æ˜¯å¦æ­£ç¡®
"""
import torch
import numpy as np
from src.utils import load_config, set_seed, get_device
from src.envs import EnvWrapper
from src.models import MADDPGNetworks
from src.algos import MADDPG
from src.buffer import ReplayBuffer
from src.trainer import MADDPGTrainer


def test_basic_components():
    """æµ‹è¯•åŸºæœ¬ç»„ä»¶"""
    print("="*60)
    print("æµ‹è¯•1: åŸºæœ¬ç»„ä»¶åˆå§‹åŒ–")
    print("="*60)
    
    # è®¾ç½®éšæœºç§å­
    set_seed(42)
    
    # åŠ è½½é…ç½®
    config = load_config('config.yaml')
    device = get_device()
    
    print(f"âœ… é…ç½®åŠ è½½æˆåŠŸ")
    print(f"âœ… è®¾å¤‡: {device}")
    
    # åˆ›å»ºç¯å¢ƒ
    env = EnvWrapper(config)
    env_info = env.get_env_info()
    
    print(f"âœ… ç¯å¢ƒåˆ›å»ºæˆåŠŸ: {env_info['n_agents']}ä¸ªæ™ºèƒ½ä½“")
    
    # åˆ›å»ºç½‘ç»œ
    networks = MADDPGNetworks(env_info, config, device)
    print(f"âœ… ç½‘ç»œåˆ›å»ºæˆåŠŸ")
    
    # åˆ›å»ºç®—æ³•
    algorithm = MADDPG(networks, config, device)
    print(f"âœ… ç®—æ³•åˆ›å»ºæˆåŠŸ")
    
    # åˆ›å»ºç¼“å†²åŒº
    buffer = ReplayBuffer(
        capacity=100,
        n_agents=env_info['n_agents'],
        obs_dim=env_info['obs_dims'][0],
        state_dim=0,
        device=device
    )
    print(f"âœ… ç¼“å†²åŒºåˆ›å»ºæˆåŠŸ")
    
    env.close()
    print("\nâœ… åŸºæœ¬ç»„ä»¶æµ‹è¯•é€šè¿‡!\n")


def test_forward_pass():
    """æµ‹è¯•å‰å‘ä¼ æ’­"""
    print("="*60)
    print("æµ‹è¯•2: å‰å‘ä¼ æ’­")
    print("="*60)
    
    set_seed(42)
    config = load_config('config.yaml')
    device = get_device()
    
    # åˆ›å»ºç¯å¢ƒå’Œç½‘ç»œ
    env = EnvWrapper(config)
    env_info = env.get_env_info()
    networks = MADDPGNetworks(env_info, config, device)
    algorithm = MADDPG(networks, config, device)
    
    # é‡ç½®ç¯å¢ƒ
    obs, _ = env.reset()
    
    # è½¬æ¢è§‚æµ‹ä¸ºtensor
    obs_tensor = {}
    for agent_id, agent_obs in obs.items():
        obs_tensor[agent_id] = torch.FloatTensor(agent_obs).to(device)
    
    # é€‰æ‹©åŠ¨ä½œ
    actions = algorithm.select_actions(obs_tensor, noise_scale=0.1)
    
    print(f"âœ… åŠ¨ä½œé€‰æ‹©æˆåŠŸ: {actions}")
    
    # æ‰§è¡ŒåŠ¨ä½œ
    next_obs, rewards, dones, _ = env.step(actions)
    
    print(f"âœ… ç¯å¢ƒæ­¥è¿›æˆåŠŸ")
    print(f"   å¥–åŠ±: {rewards}")
    print(f"   ç»ˆæ­¢: {dones}")
    
    env.close()
    print("\nâœ… å‰å‘ä¼ æ’­æµ‹è¯•é€šè¿‡!\n")


def test_training_step():
    """æµ‹è¯•è®­ç»ƒæ­¥éª¤"""
    print("="*60)
    print("æµ‹è¯•3: è®­ç»ƒæ­¥éª¤")
    print("="*60)
    
    set_seed(42)
    config = load_config('config.yaml')
    device = get_device()
    
    # åˆ›å»ºç¯å¢ƒå’Œç½‘ç»œ
    env = EnvWrapper(config)
    env_info = env.get_env_info()
    networks = MADDPGNetworks(env_info, config, device)
    algorithm = MADDPG(networks, config, device)
    
    # åˆ›å»ºç¼“å†²åŒº
    buffer = ReplayBuffer(
        capacity=100,
        n_agents=env_info['n_agents'],
        obs_dim=env_info['obs_dims'][0],
        state_dim=0,
        device=device
    )
    
    # æ”¶é›†ä¸€äº›ç»éªŒ
    print("æ”¶é›†ç»éªŒ...")
    for _ in range(10):
        obs, _ = env.reset()
        
        for step in range(20):
            # è½¬æ¢è§‚æµ‹
            obs_tensor = {}
            for agent_id, agent_obs in obs.items():
                obs_tensor[agent_id] = torch.FloatTensor(agent_obs).to(device)
            
            # é€‰æ‹©åŠ¨ä½œ
            actions = algorithm.select_actions(obs_tensor, noise_scale=0.1)
            
            # æ‰§è¡ŒåŠ¨ä½œ
            next_obs, rewards, dones, _ = env.step(actions)
            
            # å­˜å‚¨ç»éªŒ
            buffer.push(obs, actions, rewards, next_obs, dones,
                       np.zeros(1), np.zeros(1))
            
            obs = next_obs
            
            if all(dones.values()):
                break
    
    print(f"âœ… æ”¶é›†äº† {len(buffer)} æ¡ç»éªŒ")
    
    # é‡‡æ ·å¹¶è®­ç»ƒ
    if len(buffer) >= 64:
        batch = buffer.sample(64)
        loss_info = algorithm.update(batch)
        
        print(f"âœ… è®­ç»ƒæ­¥éª¤æˆåŠŸ")
        print(f"   CriticæŸå¤±: {loss_info['critic_loss']:.4f}")
        print(f"   ActoræŸå¤±: {loss_info['actor_loss']:.4f}")
        print(f"   æ€»æŸå¤±: {loss_info['total_loss']:.4f}")
    
    env.close()
    print("\nâœ… è®­ç»ƒæ­¥éª¤æµ‹è¯•é€šè¿‡!\n")


def test_full_training():
    """æµ‹è¯•å®Œæ•´è®­ç»ƒæµç¨‹ï¼ˆå°‘é‡episodesï¼‰"""
    print("="*60)
    print("æµ‹è¯•4: å®Œæ•´è®­ç»ƒæµç¨‹ (10 episodes)")
    print("="*60)
    
    set_seed(42)
    
    # ä¿®æ”¹é…ç½®ä¸ºå¿«é€Ÿæµ‹è¯•
    config = load_config('config.yaml')
    config['training']['total_episodes'] = 10
    config['training']['warmup_episodes'] = 2
    config['training']['eval_interval'] = 5
    config['training']['save_interval'] = 999999  # ä¸ä¿å­˜
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = MADDPGTrainer(config)
    
    # è®­ç»ƒ
    episode_rewards = trainer.train()
    
    print(f"\nâœ… è®­ç»ƒå®Œæˆ!")
    print(f"   å®Œæˆepisodes: {len(episode_rewards)}")
    print(f"   å¹³å‡å¥–åŠ±: {np.mean(episode_rewards):.2f}")
    
    trainer.close()
    print("\nâœ… å®Œæ•´è®­ç»ƒæµç¨‹æµ‹è¯•é€šè¿‡!\n")


def main():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("\n" + "="*60)
    print("MADDPG å®ç°æµ‹è¯•")
    print("="*60 + "\n")
    
    try:
        # æµ‹è¯•1: åŸºæœ¬ç»„ä»¶
        test_basic_components()
        
        # æµ‹è¯•2: å‰å‘ä¼ æ’­
        test_forward_pass()
        
        # æµ‹è¯•3: è®­ç»ƒæ­¥éª¤
        test_training_step()
        
        # æµ‹è¯•4: å®Œæ•´è®­ç»ƒ
        test_full_training()
        
        print("="*60)
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡!")
        print("="*60)
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return 1
    
    return 0


if __name__ == "__main__":
    exit(main())
