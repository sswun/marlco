# MADDPG (Multi-Agent Deep Deterministic Policy Gradient)

## ç®—æ³•ç®€ä»‹

MADDPGï¼ˆå¤šæ™ºèƒ½ä½“æ·±åº¦ç¡®å®šæ€§ç­–ç•¥æ¢¯åº¦ï¼‰æ˜¯ä¸€ç§ä¸“ä¸ºå¤šæ™ºèƒ½ä½“ç¯å¢ƒè®¾è®¡çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ã€‚å®ƒé€šè¿‡**"é›†ä¸­å¼è®­ç»ƒï¼Œåˆ†å¸ƒå¼æ‰§è¡Œ"ï¼ˆCTDEï¼‰**çš„æ¡†æ¶ï¼Œæœ‰æ•ˆè§£å†³äº†å¤šæ™ºèƒ½ä½“ç³»ç»Ÿä¸­çš„éå¹³ç¨³æ€§é—®é¢˜ã€‚

### æ ¸å¿ƒæ€æƒ³

1. **é›†ä¸­å¼è®­ç»ƒ**: è®­ç»ƒæ—¶ï¼Œæ¯ä¸ªæ™ºèƒ½ä½“çš„Criticç½‘ç»œå¯ä»¥è®¿é—®æ‰€æœ‰æ™ºèƒ½ä½“çš„è§‚æµ‹å’ŒåŠ¨ä½œï¼Œä»è€Œè·å¾—ç¨³å®šçš„å­¦ä¹ ç¯å¢ƒ
2. **åˆ†å¸ƒå¼æ‰§è¡Œ**: æ‰§è¡Œæ—¶ï¼Œæ¯ä¸ªæ™ºèƒ½ä½“çš„Actorç½‘ç»œä»…ä¾èµ–è‡ªå·±çš„å±€éƒ¨è§‚æµ‹è¿›è¡Œå†³ç­–
3. **Actor-Criticæ¶æ„**: 
   - Actorç½‘ç»œè¾“å‡ºåŠ¨ä½œæ¦‚ç‡åˆ†å¸ƒ
   - Criticç½‘ç»œè¯„ä¼°çŠ¶æ€-åŠ¨ä½œå¯¹çš„ä»·å€¼

### ä¸QMIXçš„åŒºåˆ«

| ç‰¹æ€§ | MADDPG | QMIX |
|------|--------|------|
| **ç½‘ç»œç»“æ„** | Actor-Critic (æ¯ä¸ªæ™ºèƒ½ä½“ç‹¬ç«‹) | Q-learning + Mixing Network |
| **åŠ¨ä½œé€‰æ‹©** | ç­–ç•¥æ¢¯åº¦ï¼ˆè¿ç»­/ç¦»æ•£åŠ¨ä½œï¼‰ | Îµ-greedy Qå€¼é€‰æ‹© |
| **é›†ä¸­å¼ä¿¡æ¯** | Criticä½¿ç”¨å…¨å±€è§‚æµ‹+åŠ¨ä½œ | Mixing Networkç»„åˆä¸ªä½“Qå€¼ |
| **é€‚ç”¨åœºæ™¯** | åä½œ/ç«äº‰/æ··åˆ | ä¸»è¦é€‚ç”¨äºåä½œä»»åŠ¡ |
| **æ¢ç´¢ç­–ç•¥** | é«˜æ–¯å™ªå£°/OUå™ªå£° | Îµ-greedy |

## é¡¹ç›®ç»“æ„

```
maddpg/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py           # åŒ…åˆå§‹åŒ–
â”‚   â”œâ”€â”€ models.py             # Actorå’ŒCriticç½‘ç»œæ¨¡å‹
â”‚   â”œâ”€â”€ algos.py              # MADDPGç®—æ³•æ ¸å¿ƒå®ç°
â”‚   â”œâ”€â”€ buffer.py             # ç»éªŒå›æ”¾ç¼“å†²åŒº
â”‚   â”œâ”€â”€ envs.py               # ç¯å¢ƒåŒ…è£…å™¨
â”‚   â”œâ”€â”€ trainer.py            # è®­ç»ƒå™¨
â”‚   â”œâ”€â”€ utils.py              # å·¥å…·å‡½æ•°
â”‚   â””â”€â”€ pettingzoo_adapter.py # PettingZooç¯å¢ƒé€‚é…å™¨
â”œâ”€â”€ checkpoints/              # æ¨¡å‹æ£€æŸ¥ç‚¹ä¿å­˜ç›®å½•
â”œâ”€â”€ plots/                    # è®­ç»ƒå›¾è¡¨ä¿å­˜ç›®å½•
â”œâ”€â”€ config.yaml               # é»˜è®¤é…ç½®æ–‡ä»¶
â”œâ”€â”€ config_*.yaml             # å„ç¯å¢ƒä¸“ç”¨é…ç½®
â”œâ”€â”€ main.py                   # ä¸»è®­ç»ƒè„šæœ¬
â””â”€â”€ README.md                 # æœ¬æ–‡ä»¶
```

## å¿«é€Ÿå¼€å§‹

### 1. åŸºæœ¬è®­ç»ƒ

ä½¿ç”¨é»˜è®¤é…ç½®è®­ç»ƒCMç¯å¢ƒ:

```bash
cd maddpg
python main.py
```

### 2. æŒ‡å®šç¯å¢ƒå’Œéš¾åº¦

```bash
# DEMç¯å¢ƒï¼Œéš¾åº¦normal
python main.py --env DEM --difficulty normal

# HRGç¯å¢ƒï¼Œultra_fastç‰ˆæœ¬
python main.py --env HRG --difficulty ultra_fast

# MSFSç¯å¢ƒï¼Œéš¾åº¦hard
python main.py --env MSFS --difficulty hard
```

### 3. ä½¿ç”¨é…ç½®æ–‡ä»¶

```bash
# ä½¿ç”¨CM hardé…ç½®
python main.py --config config_CM_hard.yaml

# ä½¿ç”¨DEM normalé…ç½®
python main.py --config config_DEM_normal.yaml
```

### 4. ç”Ÿæˆè®­ç»ƒå›¾è¡¨

```bash
# è®­ç»ƒå¹¶ç”Ÿæˆå›¾è¡¨
python main.py --plots

# è®­ç»ƒå¹¶æ˜¾ç¤ºå›¾è¡¨
python main.py --show-plots

# æŒ‡å®šå›¾è¡¨ä¿å­˜ç›®å½•
python main.py --plots --plot-dir my_plots
```

### 5. è‡ªå®šä¹‰è®­ç»ƒå‚æ•°

```bash
# è®­ç»ƒ10000ä¸ªepisodes
python main.py --episodes 10000

# ä½¿ç”¨ç‰¹å®šéšæœºç§å­
python main.py --seed 123

# ç»„åˆä½¿ç”¨
python main.py --env CM --difficulty hard --episodes 8000 --plots
```

## é…ç½®è¯´æ˜

### ç¯å¢ƒé…ç½® (`env`)

```yaml
env:
  name: "CM"                    # ç¯å¢ƒåç§°: DEM, HRG, MSFS, CM, SMAC
  difficulty: "hard"           # éš¾åº¦çº§åˆ«
  global_state_type: "concat"  # å…¨å±€çŠ¶æ€ç±»å‹
```

### ç®—æ³•é…ç½® (`algorithm`)

```yaml
algorithm:
  gamma: 0.99                  # æŠ˜æ‰£å› å­
  actor_lr: 0.001             # Actorå­¦ä¹ ç‡
  critic_lr: 0.001            # Criticå­¦ä¹ ç‡
  tau: 0.005                   # ç›®æ ‡ç½‘ç»œè½¯æ›´æ–°ç³»æ•°
  max_grad_norm: 10.0         # æ¢¯åº¦è£å‰ª
```

### æ¨¡å‹é…ç½® (`model`)

```yaml
model:
  actor_hidden_dim: 256        # Actorç½‘ç»œéšè—å±‚ç»´åº¦
  critic_hidden_dim: 256       # Criticç½‘ç»œéšè—å±‚ç»´åº¦
```

### è®­ç»ƒé…ç½® (`training`)

```yaml
training:
  total_episodes: 5000        # æ€»è®­ç»ƒepisodes
  batch_size: 64              # æ‰¹å¤„ç†å¤§å°
  buffer_size: 100           # ç»éªŒå›æ”¾ç¼“å†²åŒºå¤§å°ï¼ˆÃ—1000ï¼‰
  warmup_episodes: 50        # é¢„çƒ­episodes
  eval_interval: 100          # è¯„ä¼°é—´éš”
  save_interval: 500          # ä¿å­˜é—´éš”
```

### æ¢ç´¢é…ç½® (`exploration`)

```yaml
exploration:
  noise_type: "gaussian"      # å™ªå£°ç±»å‹: gaussian, ou
  noise_scale: 0.1           # å™ªå£°æ ‡å‡†å·®
  noise_decay: 0.995         # å™ªå£°è¡°å‡ç‡
  noise_min: 0.01            # æœ€å°å™ªå£°
```

## æ”¯æŒçš„ç¯å¢ƒ

æœ¬å®ç°æ”¯æŒä»¥ä¸‹å¤šæ™ºèƒ½ä½“ç¯å¢ƒ:

### 1. è‡ªå®šä¹‰ç¯å¢ƒ
- **CM** (Cooperative Movement): åˆä½œç§»åŠ¨ä»»åŠ¡
  - éš¾åº¦: easy, normal, hard
  
- **DEM** (Disaster Emergency Management): ç¾å®³åº”æ€¥ç®¡ç†
  - éš¾åº¦: easy, normal, hard
  
- **HRG** (Hospital Resource Grid): åŒ»é™¢èµ„æºè°ƒåº¦
  - éš¾åº¦: easy, normal, hard, ultra_fast
  
- **MSFS** (Multi-Sensor Fusion System): å¤šä¼ æ„Ÿå™¨èåˆ
  - éš¾åº¦: easy, normal, hard

### 2. SMAC (StarCraft Multi-Agent Challenge)
- åœ°å›¾: 8m, 3s5z, MMM, ç­‰
- é¢„è®¾é…ç½®: easy, normal, hard, debug

### 3. PettingZooç¯å¢ƒ
- **multiwalker**: å¤šæ­¥è¡Œè€…åä½œ
- **simple_spread**: ç®€å•æ‰©æ•£
- **simple_crypto**: ç®€å•åŠ å¯†é€šä¿¡

## è®­ç»ƒè¾“å‡º

è®­ç»ƒè¿‡ç¨‹ä¸­ä¼šç”Ÿæˆä»¥ä¸‹å†…å®¹:

### 1. æ£€æŸ¥ç‚¹æ–‡ä»¶
ä¿å­˜åœ¨ `checkpoints/` ç›®å½•:
- `maddpg_episode_*.pt`: æ¨¡å‹æ£€æŸ¥ç‚¹
- `*_training_data_*.json`: è®­ç»ƒæ•°æ®

### 2. è®­ç»ƒå›¾è¡¨
ä¿å­˜åœ¨ `plots/` ç›®å½•ï¼ˆéœ€ä½¿ç”¨`--plots`å‚æ•°ï¼‰:
- Episodeå¥–åŠ±æ›²çº¿
- Episodeé•¿åº¦ç»Ÿè®¡
- è®­ç»ƒæŸå¤±æ›²çº¿
- å™ªå£°è¡°å‡æ›²çº¿
- å¥–åŠ±åˆ†å¸ƒç›´æ–¹å›¾
- æ€§èƒ½è¶‹åŠ¿åˆ†æ

### 3. æ§åˆ¶å°æ—¥å¿—
```
ğŸŒ Environment: CM
   Agents: 3
   Obs dims: [12, 12, 12]
   Action dims: [5, 5, 5]
   Device: cuda

ğŸš€ å¼€å§‹MADDPGè®­ç»ƒ...
Episode      0 | Avg Reward: -45.23 | Avg Length: 78.0 | Noise: 0.100 | Buffer:    50
Episode    100 | Avg Reward: -12.34 | Avg Length: 95.5 | Noise: 0.060 | Buffer:  5000
ğŸ¯ Evaluation at episode 100: -8.45
...
```

## ç®—æ³•ç‰¹ç‚¹

### ä¼˜åŠ¿
- âœ… æœ‰æ•ˆè§£å†³å¤šæ™ºèƒ½ä½“ç¯å¢ƒçš„éå¹³ç¨³æ€§é—®é¢˜
- âœ… æ”¯æŒåˆ†å¸ƒå¼æ‰§è¡Œï¼Œé€šä¿¡æˆæœ¬ä½
- âœ… é€‚ç”¨äºåä½œã€ç«äº‰å’Œæ··åˆå‹ä»»åŠ¡
- âœ… å¯å¤„ç†è¿ç»­å’Œç¦»æ•£åŠ¨ä½œç©ºé—´

### å±€é™
- âŒ æ™ºèƒ½ä½“æ•°é‡è¿‡å¤šæ—¶å­˜åœ¨å¯æ‰©å±•æ€§é—®é¢˜
- âŒ è®­ç»ƒæ—¶éœ€è¦è®¿é—®æ‰€æœ‰æ™ºèƒ½ä½“çš„ä¿¡æ¯
- âŒ æ ·æœ¬æ•ˆç‡ç›¸å¯¹è¾ƒä½

## ä¸QMIXçš„å¯¹æ¯”å®éªŒ

ä¸ºäº†ç®—æ³•å¯¹æ¯”ï¼ŒMADDPGä¸QMIXä½¿ç”¨ç›¸åŒçš„:
- ç¯å¢ƒé…ç½®
- è®­ç»ƒepisodesæ•°
- è¯„ä¼°é—´éš”
- æ•°æ®ä¿å­˜æ ¼å¼
- å›¾è¡¨ç”Ÿæˆæ–¹å¼

å¯ä»¥ç›´æ¥å¯¹æ¯”ä¸¤ä¸ªç®—æ³•åœ¨ç›¸åŒç¯å¢ƒä¸‹çš„æ€§èƒ½å·®å¼‚ã€‚

## å‚è€ƒæ–‡çŒ®

1. Lowe, R., Wu, Y., Tamar, A., Harb, J., Abbeel, P., & Mordatch, I. (2017). Multi-agent actor-critic for mixed cooperative-competitive environments. In Advances in Neural Information Processing Systems (pp. 6379-6390).

2. Lillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez, T., Tassa, Y., ... & Wierstra, D. (2015). Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971.

## è®¸å¯è¯

æœ¬é¡¹ç›®éµå¾ªä¸ä¸»MARLé¡¹ç›®ç›¸åŒçš„è®¸å¯è¯ã€‚
